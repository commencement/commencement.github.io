<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=7.0000" http-equiv="X-UA-Compatible">
  <title>Zili Wang's Homepage</title>
  <meta name="description" content="Zili Wang, Algorithm Engineer at Xiaohongshu Inc.">
  <meta name="keywords" content="Zili Wang, Large Language Model Pre-training">
  <link rel="stylesheet" type="text/css" href="./files/ZiliWang.css">

  <style>
    @-moz-keyframes nodeInserted {
      from {
        opacity: 0.99;
      }
      to {
        opacity: 1;
      }
    }
    @-webkit-keyframes nodeInserted {
      from {
        opacity: 0.99;
      }
      to {
        opacity: 1;
      }
    }
    @-o-keyframes nodeInserted {
      from {
        opacity: 0.99;
      }
      to {
        opacity: 1;
      }
    }
    @keyframes nodeInserted {
      from {
        opacity: 0.99;
      }
      to {
        opacity: 1;
      }
    }
    embed,
    object {
      animation-duration: .001s;
      -ms-animation-duration: .001s;
      -moz-animation-duration: .001s;
      -webkit-animation-duration: .001s;
      -o-animation-duration: .001s;
      animation-name: nodeInserted;
      -ms-animation-name: nodeInserted;
      -moz-animation-name: nodeInserted;
      -webkit-animation-name: nodeInserted;
      -o-animation-name: nodeInserted;
    }

    body {
      font-size: 18px; /* Increase base font size */
      line-height: 1.6;
      margin: 0; /* Remove default margin */
    }

#content {
    max-width: 4000px; /* 设置内容最大宽度为1200px */
    margin: 0 auto; /* 居中对齐内容 */
    padding: 20px; /* 添加填充以增加内部间距 */
}


    h2 {
      font-size: 24px;
      font-weight: bold;
    }

    .highlight {
      background-color: #ffeb3b;
      padding: 10px;
      border-radius: 5px;
      font-weight: bold;
      display: inline-block;
      margin: 20px 0;
    }

    .model-info {
      background-color: #e0f7fa;
      padding: 15px;
      border-radius: 5px;
      margin: 20px 0;
      font-size: 18px; /* Increase font size */
    }

    .model-info a {
      color: #00796b;
      font-weight: bold;
    }

    table {
      width: 100%; /* Ensure tables take full width */
    }

    td {
      padding: 10px; /* Add padding for better readability */
    }

    .title {
      font-size: 15px; /* Larger font size for titles */
      font-weight: bold; /* Make titles bold */
    }

    p {
      margin: 10px 0; /* Add margin for spacing between paragraphs */
    }

    a {
      font-weight: bold; /* Make links bold */
      text-decoration: none; /* Remove underlines for a cleaner look */
    }
  </style>

<script>
// 缓存GitHub仓库的星标数
const githubStarsCache = {};

// 获取GitHub仓库的星标数（使用公共API，无需token）
async function fetchGitHubStars(repo, elementId) {
    try {
        if (githubStarsCache[repo]) {
            document.getElementById(elementId).textContent = `⭐ ${githubStarsCache[repo]}`;
            return;
        }
        
        // 使用公共API，添加随机延迟避免限流
        await new Promise(resolve => setTimeout(resolve, Math.random() * 1000));
        
        const response = await fetch(`https://api.github.com/repos/${repo}`, {
            headers: {
                'Accept': 'application/vnd.github.v3+json',
                'User-Agent': 'Mozilla/5.0 (compatible; GitHub-Stars-Fetcher)'
            }
        });
        
        if (!response.ok) {
            if (response.status === 403) {
                // 如果遇到限流，显示默认值
                document.getElementById(elementId).textContent = '⭐ ~';
                return;
            }
            throw new Error(`HTTP error! Status: ${response.status}`);
        }
        
        const data = await response.json();
        const stars = data.stargazers_count;
        githubStarsCache[repo] = stars;
        document.getElementById(elementId).textContent = `⭐ ${stars}`;
    } catch (error) {
        console.error(`Error fetching GitHub stars for ${repo}:`, error);
        // 显示默认值而不是Error
        document.getElementById(elementId).textContent = '⭐ ~';
    }
}

// 获取Hugging Face模型的下载次数
async function fetchHuggingFaceDownloads(model, elementId) {
    try {
        const response = await fetch(`https://huggingface.co/api/models/${model}`);
        if (!response.ok) {
            throw new Error(`HTTP error! Status: ${response.status}`);
        }
        
        const data = await response.json();
        const downloads = data.downloads || 0;
        document.getElementById(elementId).textContent = `⬇️ ${downloads}`;
    } catch (error) {
        console.error(`Error fetching Hugging Face downloads for ${model}:`, error);
        document.getElementById(elementId).textContent = '⬇️ ~';
    }
}

// 批量获取GitHub仓库的星标数（添加延迟避免限流）
async function fetchMultipleGitHubStars(repos) {
    for (let i = 0; i < repos.length; i++) {
        const { repo, elementId } = repos[i];
        await fetchGitHubStars(repo, elementId);
        // 添加延迟避免GitHub API限流
        if (i < repos.length - 1) {
            await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 2000));
        }
    }
}

// 在页面加载时调用函数
document.addEventListener('DOMContentLoaded', () => {
    fetchHuggingFaceDownloads('infly/INF-34B-Base', 'inf-34b-downloads');
    fetchHuggingFaceDownloads('opencoder-llm/opencoder-llm', 'opencoder-downloads');
    
    const githubRepos = [
        { repo: 'stepfun-ai/Step3', elementId: 'step3-stars' },
        { repo: 'infly-ai/INF-LLM', elementId: 'inf-llm-stars' },
        { repo: 'infly-ai/INF-LLM', elementId: 'inf-llm-2-stars' },
        { repo: 'opencoder-llm/opencoder-llm', elementId: 'opencoder-stars' },
        { repo: 'Bumble666/Hyper_MoE', elementId: 'hyper-moe-stars' },
        { repo: 'mutonix/RefGPT', elementId: 'ref-gpt-stars' },
        { repo: 'MikeGu721/XiezhiBenchmark', elementId: 'xiezhi-benchmark-stars' },
        { repo: 'WaitHZ/GW-MoE', elementId: 'gw-moe-stars' },
        { repo: 'kamanphoebe/Look-into-MoEs', elementId: 'look-into-moes-stars' },
        { repo: 'multimodal-art-projection/MAP-NEO', elementId: 'map-neo-stars' },
        { repo: 'sanderwood/bgpt', elementId: 'bgpt-stars' },
        { repo: 'geekan/MetaGPT', elementId: 'metagpt-stars' },
        { repo: 'RuifengYuan/Prefix_Merging', elementId: 'Prefix-Merging-stars' },
        { repo: 'RuifengYuan/Sentence-Fusion-with-event-graph', elementId: 'sentence-Fusion-with-event-graph-stars' },
        { repo: 'RuifengYuan/FactExsum-coling2020', elementId: 'factExsum-coling2020-stars' }
    ];
    
    fetchMultipleGitHubStars(githubRepos);
});

</script>



</head>

<body>
  <div id="content">
    <div id="left">
      <table style="background-color:white;">
        <tbody>
          <tr nosave="">
            <td valign="CENTER">
              <img src="./images/ziliwang_new.jpg" height="250" align="left">
            </td>

            <td valign="CENTER" width="2%">
            </td>

            <td valign="CENTER" halign="LEFT">
              <font size="+0">
                <b><font size="+2">Zili Wang&nbsp;</font></b>
                <p style="margin-left:0px;">
                  <img src="./images/ziliwang_name.png" height="60">
                </p>
                <p style="margin-left:0px;">
                  <!-- <b>Algorithm Expert</b> -->
                </p>
                <p style="margin-left:0px;">
                  <!-- <a href="http://www.nextcenter.org/", target="_blank">NExT++</a><br/> -->
                  <a href="https://stepfun.com/" target="_blank">StepFun</a><br/>
                  <!-- <a href="https://www.polyu.edu.hk/", target="_blank">Hong Kong Polytechnic University</a><br/> -->
                </p>
                <p style="margin-left:0px;">
                  Shang Hai, China<br>
                </p>
                <p style="margin-left:0px;">
                  Email: ziliwang.do@gmail.com<br>
                  &bull; <a href="">CV</a> &bull; <a href="https://scholar.google.com/citations?hl=zh-CN&user=E9zWgmwAAAAJ">Google Scholar</a> &bull; <a href="https://github.com/commencement">GitHub</a> <br>
                </p>
              </font>
              <p><font size="+0">
                </font>
              </p>
            </td>
          </tr>
        </tbody>
      </table>

      <div style="margin-top:20px;">
        Zili Wang is currently an LLM Researcher at <a href="https://stepfun.com/">StepFun</a> (October 2024 - Present), focusing on large language model pretraining, particularly leading the Step3 project. Previously, he worked as an Algorithm Expert at <a href="https://www.infly.cn/">INF Technology</a> (September 2023 - September 2024), an Algorithm Engineer at Xiaohongshu Inc. (March 2022 - September 2023) with <a href="https://wangshusen.github.io/">Prof. Shusen Wang</a>, and a Research Assistant at Hong Kong Polytechnic University (February 2020 - March 2022) with <a href="https://www4.comp.polyu.edu.hk/~cswjli/">Prof. Wenjie Li</a>.
      </div>

      <h2 style="CLEAR: both;">Recent Projects</h2>

      <div class="model-info">
        <p><strong>Step3: Cost-Effective Multimodal Intelligence</strong></p>
        <p>Step3 is our cutting-edge multimodal reasoning model—built on a massive Mixture-of-Experts architecture with 321 billion total parameters and 38 billion active. It is designed end-to-end to minimize decoding costs while delivering top-tier performance in vision–language reasoning, mathematics, and code. Through the co-design of Multi-Matrix Factorization Attention (MFA) and Attention-FFN Disaggregation (AFD), Step3 maintains exceptional efficiency across both flagship and low-end accelerators.</p>
        <p>Links:
          <a href="https://github.com/stepfun-ai/Step3" target="_blank">
            <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="width: 16px; height: 16px; vertical-align: middle;"> GitHub
          </a> 
          <span id="step3-stars"></span>
          |
          <a href="https://stepfun.com/" target="_blank">
            <img src="https://cdn-icons-png.flaticon.com/512/1006/1006771.png" alt="Project Page" style="width: 16px; height: 16px; vertical-align: middle;"> Project Page
          </a>
          |
          <a href="https://arxiv.org/abs/2507.19427" target="_blank">
            <img src="https://upload.wikimedia.org/wikipedia/commons/8/87/PDF_file_icon.svg" alt="Paper" style="width: 16px; height: 16px; vertical-align: middle;"> Paper
          </a>
        </p>
      </div>
      <div class="model-info">
        <p><strong>OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models</strong></p>
        <p>OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B base and chat models, supporting both English and Chinese languages. Starting from scratch, OpenCoder is trained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, reaching the performance of top-tier code LLMs. We provide not only model weights and inference code, but also the reproducible training data, the complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols.</p>
        <p>Links:
          <a href="https://github.com/opencoder-llm/opencoder-llm" target="_blank">
            <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="width: 16px; height: 16px; vertical-align: middle;"> GitHub
          </a> 
          <span id="opencoder-stars"></span>
          |
          <a href="https://huggingface.co/opencoder-llm" target="_blank">
            <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="HuggingFace" style="width: 16px; height: 16px; vertical-align: middle;"> HuggingFace
          </a> 
          <span id="opencoder-downloads"></span>
          |
          <a href="https://opencoder-llm.github.io/" target="_blank">
            <img src="https://cdn-icons-png.flaticon.com/512/1006/1006771.png" alt="Project Page" style="width: 16px; height: 16px; vertical-align: middle;"> Project Page
          </a>
          |
          <a href="https://arxiv.org/pdf/2411.04905" target="_blank">
            <img src="https://upload.wikimedia.org/wikipedia/commons/8/87/PDF_file_icon.svg" alt="Paper" style="width: 16px; height: 16px; vertical-align: middle;"> Paper
          </a>
        </p>
      </div>


      <div class="model-info">
        <p><strong>INF-34B: INF’s Open-Source Large Language Models</strong></p>
        <p>INF-34B has 34 billion parameters with a context window length of 32K, and is trained on about 3.5T well-processed tokens from English and Chinese bilingual corpus. Compared with open source models of comparable size, INF-34B not only provides competitive performance in the OpenCompass evaluation, but also has impressive potential in both finance and healthcare domains. Besides, the quantized INF-34B runs on graphics cards of 24GB VRAM with negligible accuracy loss, which facilitates commercial applications, especially low-resource scenarios.</p>
        <p>Links:
          <a href="https://github.com/infly-ai/INF-LLM" target="_blank">
            <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="width: 16px; height: 16px; vertical-align: middle;"> GitHub
          </a> 
          <span id="inf-llm-stars"></span>
          |
          <a href="https://huggingface.co/infly/INF-34B-Base" target="_blank">
            <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="HuggingFace" style="width: 16px; height: 16px; vertical-align: middle;"> HuggingFace
          </a> 
          <span id="inf-34b-downloads"></span>
          |
          <a href="https://s.infly.cn/f/img/pdf/inf_34b_tech_report.pdf" target="_blank">
            <img src="https://upload.wikimedia.org/wikipedia/commons/8/87/PDF_file_icon.svg" alt="Tech Report" style="width: 16px; height: 16px; vertical-align: middle;"> Tech Report
          </a>
        </p>
      </div>



      <h2 style="CLEAR: both">Large Language Model Pretraining Related Publications </h2>
      </br>

      <table>
        <tbody>
               <tr>
                <td class="left">
                    <a href="https://s.infly.cn/f/img/pdf/inf_34b_tech_report.pdf" target="_blank">
                        <img src="./images/pdf.png" width="25" height="25"><br>pdf
                    </a>
                </td>
                <td>
                    <span class="title">INF’s Open-Source Large Language Models</span>
                    <br>Jiaran Hao, <b>Zili Wang</b>, LiuYihan Song, Ansheng You, Zhipeng Zhou, Xiaoyu Tan, Dakuan Lu, Xiaoming Shi, Chao Qu, Haozhe Wang, Yinghui Xu, Wei Chu, Yuan Qi
                    <!-- <br>&nbsp;&nbsp;&bull; <a href="https://s.infly.cn/f/img/pdf/inf_34b_tech_report.pdf" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                    <br>&bull; <a href="https://github.com/infly-ai/INF-LLM/" target="_blank">Github</a> <span id="inf-llm-2-stars"></span> &nbsp;&nbsp;
                    &bull; <a href="https://huggingface.co/infly/INF-34B-Base/" target="_blank">Huggingface</a> &nbsp;&nbsp;
                </td>
            </tr>


                  <tr>
                <td class="left">
                    <a href="https://arxiv.org/pdf/2405.19327" target="_blank">
                        <img src="./images/pdf.png" width="25" height="25"><br>pdf
                    </a>
                </td>
                <td>
                    <span class="title">Map-neo: Highly capable and transparent bilingual large language model series</span>
                    <br>Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, <b>Zenith Wang</b>, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen
                    <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2405.19327" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                    <br>&bull; <a href="https://github.com/multimodal-art-projection/MAP-NEO" target="_blank">Github</a> <span id="map-neo-stars"></span> &nbsp;&nbsp;

                </td>
            </tr>

            <tr>
                <td class="left">
                    <a href="https://arxiv.org/pdf/2402.12656" target="_blank">
                        <img src="./images/pdf.png" width="25" height="25"><br>pdf
                    </a>
                </td>
                <td>
                    <span class="title">HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts</span>
                    <br>Hao Zhao, Zihan Qiu, Huijia Wu, <b>Zili Wang</b>, Zhaofeng He, Jie Fu
                    <br>ACL 2024
                    <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/pdf/2402.12656" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                    <br>&bull; <a href="https://github.com/Bumble666/Hyper_MoE" target="_blank">Github</a> <span id="hyper-moe-stars"></span> &nbsp;&nbsp;
                </td>
            </tr>

            <tr>
                <td class="left">
                    <a href="https://arxiv.org/pdf/2305.14994.pdf" target="_blank">
                        <img src="./images/pdf.png" width="25" height="25"><br>pdf
                    </a>
                </td>
                <td>
                    <span class="title">RefGPT: Reference-> Truthful & Customized Dialogues Generation by GPTs and for GPTs</span>
                    <br>Dongjie Yang, Ruifeng Yuan, YuanTao Fan, YiFei Yang, <b>Zili Wang</b>, Shushen Wang, Hai Zhao
                    <br>EMNLP 2023
                    <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2305.14994" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                    <br>&bull; <a href="https://github.com/mutonix/RefGPT" target="_blank">Github</a> <span id="ref-gpt-stars"></span> &nbsp;&nbsp;
                </td>
            </tr>

            <tr>
                <td class="left">
                    <a href="https://arxiv.org/pdf/2306.05783.pdf" target="_blank">
                        <img src="./images/pdf.png" width="25" height="25"><br>pdf
                    </a>
                </td>
                <td>
                    <span class="title">Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation</span>
                    <br>Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu He, Rui Xu, Wenhao Huang, <b>Zili Wang</b>, Shusen Wang, Weiguo Zheng, Hongwei Feng, Yanghua Xiao
                    <br>AAAI 2024
                    <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2306.05783" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                    <br>&bull; <a href="https://github.com/MikeGu721/XiezhiBenchmark" target="_blank">Github</a> <span id="xiezhi-benchmark-stars"></span> &nbsp;&nbsp;
                </td>
            </tr>

            <tr>
                <td class="left">
                    <a href="https://arxiv.org/abs/2406.12375" target="_blank">
                        <img src="./images/pdf.png" width="25" height="25"><br>pdf
                    </a>
                </td>
                <td>
                    <span class="title">GW-MoE: Resolving Uncertainty in MoE Router with Global Workspace Theory</span>
                    <br>Haoze Wu, Zihan Qiu, <b>Zili Wang</b>, Hang Zhao, Jie Fu
                    <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2406.12375" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                    <br>&bull; <a href="https://github.com/WaitHZ/GW-MoE" target="_blank">Github</a> <span id="gw-moe-stars"></span> &nbsp;&nbsp;
                </td>
            </tr>

            <tr>
                <td class="left">
                    <a href="https://arxiv.org/pdf/2406.18219" target="_blank">
                        <img src="./images/pdf.png" width="25" height="25"><br>pdf
                    </a>
                </td>
                <td>
                    <span class="title">A Closer Look into Mixture-of-Experts in Large Language Models</span>
                    <br>Ka Man Lo, Zeyu Huang, Zihan Qiu, <b>Zili Wang</b>, Jie Fu
                    <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2406.18219" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                    <br>&bull; <a href="https://github.com/kamanphoebe/Look-into-MoEs" target="_blank">Github</a> <span id="look-into-moes-stars"></span> &nbsp;&nbsp;

                </td>
            </tr>

            <tr>
                <td class="left">
                    <a href="https://arxiv.org/pdf/2312.17257" target="_blank">
                        <img src="./images/pdf.png" width="25" height="25"><br>pdf
                    </a>
                </td>
                <td>
                    <span class="title">Evolving Large Language Model Assistant with Long-Term Conditional Memory</span>
                    <br>Ruifeng Yuan, Shichao Sun, <b>Zili Wang</b>, Ziqiang Cao, Wenjie Li
                    <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/pdf/2312.17257" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                    <br>&bull; <a href="https://arxiv.org/pdf/2312.17257" target="_blank">Github</a> &nbsp;&nbsp;
                </td>
            </tr>


        </tbody>
      </table>

      <h2 style="CLEAR: both">Multimodal model Related Publications </h2>
      <table>
        <tbody>
          </br>
          <tr>
            <td class="left"><a href="https://arxiv.org/abs/2402.19155" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
            <td><span class="title">Beyond Language Models: Byte Models are Digital World Simulators</span> 
              <br>Shangda Wu, Xu Tan, <b>Zili Wang</b>, Rui Wang, Xiaobing Li, Maosong Sun
              <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2402.19155" target="_blank">arXiv</a> &nbsp;&nbsp; -->
              <br>&bull; <a href="https://github.com/sanderwood/bgpt" target="_blank">Github</a> <span id="bgpt-stars"></span> &nbsp;&nbsp;

            </td>
          </tr>

          <tr>
            <td class="left"><a href="https://arxiv.org/pdf/2406.07588" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
            <td><span class="title">AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning</span> 
              <br>Jun Gao, Qian Qiao, Ziqiang Cao, <b>Zili Wang</b>, Wenjie Li
              <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2406.07588" target="_blank">arXiv</a> &nbsp;&nbsp; -->
              <br>&bull; <a href="https://commencement.github.io/" target="_blank">Github</a> &nbsp;&nbsp;

            </td>
          </tr>
        </tbody>
      </table>

      <h2 style="CLEAR: both">LLM-based Agent Related  Publications </h2>
      <table>
        <tbody>
          <tr>
            <td class="left"><a href="https://arxiv.org/pdf/2308.00352.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
            <td><span class="title">Metagpt: Meta programming for multi-agent collaborative framework</span> 
              <br>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, <b>Zili Wang</b>, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu
              <br>ICLR 2024
              <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2308.00352" target="_blank">arXiv</a> &nbsp;&nbsp; -->
              <br>&bull; <a href="https://github.com/geekan/MetaGPT" target="_blank">Github</a> <span id="metagpt-stars"></span>&nbsp;&nbsp;
            </td>
          </tr>
        </tbody>
      </table>

      <h2 style="CLEAR: both">Text Summarization Related  Publications </h2>
      <table>
        <tbody>
          <tr>
            <tr>
              <td class="left"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/29836" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
              <td><span class="title">QuerySum: A Multi-Document Query-Focused Summarization Dataset</span> 
                <br>Yushan Liu, <b>Zili Wang</b>, Ruifeng Yuan
                <br>AAAI 2024
                <!-- <br>&nbsp;&nbsp;&bull; <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29836" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                <br>&bull; <a href="" target="_blank">Github</a> &nbsp;&nbsp;
              </td>
            </tr>

            <tr>
              <td class="left"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26631" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
              <td><span class="title">Preserve Context Information through Interpretability for Extract-Generate Long-Input Summarization Framework</span> 
                <br>Ruifeng Yuan,  <b>Zili Wang</b>,  Ziqiang Cao, Wenjie Li
                <br>AAAI 2023
                <!-- <br>&nbsp;&nbsp;&bull; <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26631" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                <br>&bull; <a href="https://commencement.github.io/" target="_blank">Github</a> &nbsp;&nbsp;
              </td>
            </tr>

            <tr>
              <td class="left"><a href="https://arxiv.org/pdf/2211.16164.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
              <td><span class="title">Few-shot Query-Focused Summarization with Prefix-Merging</span> 
                <br>Ruifeng Yuan,  <b>Zili Wang</b>,  Wenjie Li
                <br>EMNLP 2022
                <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2211.16164" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                <br>&bull; <a href="https://github.com/RuifengYuan/Prefix_Merging" target="_blank">Github</a> <span id="Prefix-Merging-stars"></span>&nbsp;&nbsp;
              </td>
            </tr>

            <tr>
              <td class="left"><a href="https://aclanthology.org/2021.emnlp-main.334.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
              <td><span class="title">Event Graph based Sentence Fusion</span> 
                <br>Ruifeng Yuan*,  <b>Zili Wang</b>*<b>(Equal Contribution)</b>,  Wenjie Li
                <br>EMNLP 2021 
                <!-- <br>&nbsp;&nbsp;&bull; <a href="https://aclanthology.org/2021.emnlp-main.334.pdf" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                <br>&bull; <a href="https://github.com/RuifengYuan/Sentence-Fusion-with-event-graph" target="_blank">Github</a> <span id="sentence-fusion-with-event-graph-stars"></span>&nbsp;&nbsp;
              </td>
            </tr>

            <tr>
              <td class="left"><a href="files/Fact-level Extractive Summarization withHierarchical Graph Mask on BERT.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
              <td><span class="title">Fact level Extractive Summarization with Hierarchical Graph Mask on BERT</span> 
                <br>Ruifeng Yuan,  <b>Zili Wang</b>,  Wenjie Li
                <br>COLING 2020 
                <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2011.09739" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                <br>&bull; <a href="https://github.com/RuifengYuan/FactExsum-coling2020" target="_blank">Github</a> &nbsp;&nbsp;
              </td>
            </tr>

            <tr>
              <td class="left"><a href="files/CIKM_2020_Tip_Generation.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
              <td><span class="title">Query-aware Tip Generation for Vertical Search</span> 
                <br>Yang Yang*, Junmei Hao*, Canjia Li*, <b>Zili Wang</b>*<b>(Equal Contribution)</b>, Jingang Wang, Fuzheng Zhang, Rao Fu, Peixu Hou, Gong Zhang, Zhongyuan Wang
                <br>CIKM 2020 
                <!-- <br>&nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2010.09254" target="_blank">arXiv</a> &nbsp;&nbsp; -->
                <br>&bull; <a href="https://commencement.github.io/" target="_blank">Github</a> &nbsp;&nbsp;
                &bull; <a href="files/cikm-tip-generation.pdf" target="_blank">Slides</a> &nbsp;&nbsp;
              </td>
            </tr>
          </tr>
        </tbody>
      </table>

      <b> Look for the full publication list? Please see <a href="" target="_blank">my CV</a> or visit <a href="https://scholar.google.com/citations?hl=zh-CN&user=E9zWgmwAAAAJ" target="_blank">Google Scholar</a>.</b></br></br>

      <h2 style="CLEAR: both;">Professional Services</h2>
      <br>
      Program Committee Member of <span class="title">ACM CIKM (2023) </span> <br>
      Program Committee Member of <span class="title">ACM CIKM (2022) </span> <br>
      Program Committee Member of <span class="title">ACM CIKM (2021) </span> <br>
      </td></tr></tbody></table>

      </br>

      <h2 style="CLEAR: both;"></h2>
      <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=P5t3EabrzZY8aFh3ZhuRPAXXUh7jCpV3TVHKUlqbMjA&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
    </div>
  </div>
</body>
</html>
